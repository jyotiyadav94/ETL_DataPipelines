# Project Repositories

This README provides an overview of various projects focused on creating and managing ETL pipelines using different technologies and tools.

## 1. ETL_pipeline-main

**Description**: This project demonstrates how to create simple ETL pipelines from scratch using various technologies.

**Technologies Used**:
- Dataset (Extract, Load & Transformation)
- Environment Variables (.env)
- Requirements.txt
- MongoDB
- Dockerfile & Docker-compose
- FastAPI Web Application
- Google Cloud Platform (Deployment)
- Pytests

**Repository**: [ETL_pipeline-main](https://github.com/jyotiyadav94/ETL_DataPipelines/tree/main/ETL_pipeline-main)

---

## 2. Data Pipeline with Reddit, Airflow, Celery, PostgreSQL, S3, AWS Glue, Athena, and Redshift

**Description**: This project focuses on building a data pipeline that extracts data from Reddit using its API, stores raw data into an S3 bucket using Airflow, transforms the data using AWS Glue and Amazon Athena, and loads the transformed data into Amazon Redshift for analytics and querying.

**Steps**:
1. Extract data from Reddit using its API.
2. Store the raw data into an S3 bucket from Airflow.
3. Transform the data using AWS Glue and Amazon Athena.
4. Load the transformed data into Amazon Redshift for analytics and querying.

**Repository**: [Data Pipeline with Reddit, Airflow, Celery, PostgreSQL, S3, AWS Glue, Athena, and Redshift](https://github.com/jyotiyadav94/ETL_DataPipelines/tree/main/End-to-End-DataEngineeringPipeline)

---

## 3. etl_pipeline_apacheAirflow-main

**Description**: This project illustrates the creation of a simple ETL pipeline using Apache Airflow.

**Repository**: [etl_pipeline_apacheAirflow-main](https://github.com/jyotiyadav94/ETL_DataPipelines/tree/main/etl_pipeline_apacheAirflow-main)

---

Feel free to explore the repositories for more detailed information on each project. If you have any questions or need further information, please contact me at [jojoyadav255@gmail.com].
